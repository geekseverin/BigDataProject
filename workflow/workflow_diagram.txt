====================================================
        DIAGRAMME DE WORKFLOW - PROJET BIG DATA
====================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ARCHITECTURE GLOBALE                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA SOURCES  â”‚â”€â”€â”€â–¶â”‚   INGESTION     â”‚â”€â”€â”€â–¶â”‚   PROCESSING    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ CSV Files     â”‚    â”‚ â€¢ HDFS Upload   â”‚    â”‚ â€¢ Apache Pig    â”‚
â”‚ â€¢ MongoDB       â”‚    â”‚ â€¢ MongoDB Load  â”‚    â”‚ â€¢ Apache Spark  â”‚
â”‚ â€¢ Streaming     â”‚    â”‚ â€¢ Batch Process â”‚    â”‚ â€¢ MapReduce     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FLASK WEB     â”‚â—€â”€â”€â”€â”‚   STORAGE       â”‚â—€â”€â”€â”€â”‚   ANALYTICS     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Dashboard     â”‚    â”‚ â€¢ HDFS Results  â”‚    â”‚ â€¢ Statistics    â”‚
â”‚ â€¢ Visualizationsâ”‚    â”‚ â€¢ MongoDB Cache â”‚    â”‚ â€¢ Aggregations  â”‚
â”‚ â€¢ REST APIs     â”‚    â”‚ â€¢ File System   â”‚    â”‚ â€¢ ML Insights   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

====================================================
           WORKFLOW DÃ‰TAILLÃ‰ Ã‰TAPE PAR Ã‰TAPE
====================================================

PHASE 1: INITIALISATION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ğŸ³ Docker Compose Build & Start                          â”‚
â”‚    â”œâ”€â”€ hadoop-master (NameNode + ResourceManager)          â”‚
â”‚    â”œâ”€â”€ hadoop-secondary (SecondaryNameNode)                â”‚
â”‚    â”œâ”€â”€ hadoop-worker-1,2,3 (DataNode + NodeManager)       â”‚
â”‚    â”œâ”€â”€ spark-master + spark-worker-1,2                     â”‚
â”‚    â”œâ”€â”€ mongodb (Database + MongoDB Express)                â”‚
â”‚    â””â”€â”€ web-app (Flask Application)                         â”‚
â”‚                                                            â”‚
â”‚ 2. ğŸ”§ Services Initialization                              â”‚
â”‚    â”œâ”€â”€ Hadoop HDFS Format & Start                         â”‚
â”‚    â”œâ”€â”€ Spark Cluster Configuration                        â”‚
â”‚    â”œâ”€â”€ MongoDB Database & Collections Setup               â”‚
â”‚    â””â”€â”€ Network & Volume Configuration                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 2: INGESTION DES DONNÃ‰ES
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ğŸ“ CSV Data Ingestion                                   â”‚
â”‚    sample_data.csv â”€â”€â”€â–¶ HDFS:/data/input/                 â”‚
â”‚    sales_data.csv  â”€â”€â”€â–¶ HDFS:/data/input/                 â”‚
â”‚                                                            â”‚
â”‚ 2. ğŸƒ MongoDB Data Ingestion                               â”‚
â”‚    JSON Documents â”€â”€â”€â–¶ MongoDB Collections                â”‚
â”‚    â”œâ”€â”€ sales (transactions avec mÃ©tadonnÃ©es)              â”‚
â”‚    â”œâ”€â”€ customers (profils clients)                        â”‚
â”‚    â””â”€â”€ products (catalogue produits)                      â”‚
â”‚                                                            â”‚
â”‚ 3. ğŸ” Data Validation & Schema Check                       â”‚
â”‚    â”œâ”€â”€ VÃ©rification intÃ©gritÃ© des donnÃ©es                 â”‚
â”‚    â”œâ”€â”€ Nettoyage des valeurs nulles/aberrantes           â”‚
â”‚    â””â”€â”€ Standardisation des formats                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 3: TRAITEMENT AVEC APACHE PIG
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ğŸ· Employee Data Analysis (data_analysis.pig)           â”‚
â”‚    HDFS:/data/input/sample_data.csv                       â”‚
â”‚    â”œâ”€â”€ Statistiques par dÃ©partement                       â”‚
â”‚    â”œâ”€â”€ Analyse des tranches d'Ã¢ge                         â”‚
â”‚    â”œâ”€â”€ RÃ©partition salariale                              â”‚
â”‚    â”œâ”€â”€ Analyse gÃ©ographique (villes)                      â”‚
â”‚    â””â”€â”€ Top 10 employÃ©s les mieux payÃ©s                    â”‚
â”‚    Results â”€â”€â”€â–¶ HDFS:/data/output/pig/                    â”‚
â”‚                                                            â”‚
â”‚ 2. ğŸ· Sales Data Analysis (sales_analysis.pig)            â”‚
â”‚    HDFS:/data/input/sales_data.csv                        â”‚
â”‚    â”œâ”€â”€ KPI globaux de ventes                              â”‚
â”‚    â”œâ”€â”€ Performance par catÃ©gorie produit                  â”‚
â”‚    â”œâ”€â”€ Analyse rÃ©gionale                                  â”‚
â”‚    â”œâ”€â”€ Performance des vendeurs                           â”‚
â”‚    â”œâ”€â”€ Tendances temporelles                              â”‚
â”‚    â””â”€â”€ DÃ©tection d'anomalies                              â”‚
â”‚    Results â”€â”€â”€â–¶ HDFS:/data/output/pig/                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 4: TRAITEMENT AVEC APACHE SPARK
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. âš¡ Advanced Data Processing (data_processing.py)        â”‚
â”‚    HDFS:/data/input/*.csv                                 â”‚
â”‚    â”œâ”€â”€ Window Functions pour analyses avancÃ©es            â”‚
â”‚    â”œâ”€â”€ Segmentation de performance employÃ©s               â”‚
â”‚    â”œâ”€â”€ Calculs de percentiles et rankings                 â”‚
â”‚    â”œâ”€â”€ Analyses cohort et RFM                             â”‚
â”‚    â””â”€â”€ Machine Learning prÃ©paratoire                      â”‚
â”‚    Results â”€â”€â”€â–¶ HDFS:/data/output/spark/                  â”‚
â”‚                                                            â”‚
â”‚ 2. âš¡ MongoDB Integration (mongodb_reader.py)              â”‚
â”‚    MongoDB:bigdata.* â—€â”€â”€â”€â–¶ Spark DataFrames              â”‚
â”‚    â”œâ”€â”€ Lecture collections MongoDB                        â”‚
â”‚    â”œâ”€â”€ Transformations complexes                          â”‚
â”‚    â”œâ”€â”€ AgrÃ©gations multi-dimensionnelles                 â”‚
â”‚    â”œâ”€â”€ Jointures cross-platform                          â”‚
â”‚    â””â”€â”€ Sauvegarde rÃ©sultats vers MongoDB                 â”‚
â”‚    Results â”€â”€â”€â–¶ MongoDB:bigdata.results                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 5: VISUALISATION ET APIs
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ğŸŒ Flask Web Application                                â”‚
â”‚    â”œâ”€â”€ Route: / (Homepage with overview)                  â”‚
â”‚    â”œâ”€â”€ Route: /dashboard (Advanced analytics)             â”‚
â”‚    â”œâ”€â”€ API: /api/sales/by-region                          â”‚
â”‚    â”œâ”€â”€ API: /api/sales/by-category                        â”‚
â”‚    â”œâ”€â”€ API: /api/analytics/pig-results                    â”‚
â”‚    â”œâ”€â”€ API: /api/analytics/spark-results                  â”‚
â”‚    â”œâ”€â”€ API: /api/mongodb/collections                      â”‚
â”‚    â””â”€â”€ API: /api/system/status                            â”‚
â”‚                                                            â”‚
â”‚ 2. ğŸ“Š Data Visualization                                   â”‚
â”‚    â”œâ”€â”€ Chart.js pour graphiques interactifs              â”‚
â”‚    â”œâ”€â”€ Bootstrap pour interface responsive                â”‚
â”‚    â”œâ”€â”€ Tableaux dynamiques avec pagination               â”‚
â”‚    â”œâ”€â”€ Export CSV/JSON                                     â”‚
â”‚    â””â”€â”€ Monitoring temps rÃ©el                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

====================================================
               FLUX DE DONNÃ‰ES DÃ‰TAILLÃ‰
====================================================

1. DATA INGESTION FLOW
   CSV Files â”€â”€â”
               â”œâ”€â–¶ HDFS â”€â”€â”
   MongoDB â”€â”€â”€â”€â”¤          â”œâ”€â–¶ Processing Layer
   Streaming â”€â”€â”˜          â”‚
                         â–¼
2. PROCESSING FLOW        
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚     Pig Scripts     â”‚ Spark Scripts â”‚
   â”‚ â”œâ”€ Descriptive     â”‚ â”œâ”€ Predictive â”‚
   â”‚ â”œâ”€ Aggregation    â”‚ â”œâ”€ ML Ready   â”‚
   â”‚ â””â”€ Statistical    â”‚ â””â”€ Real-time  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
3. RESULTS STORAGE
   â”Œâ”€ HDFS:/data/output/pig/
   â”œâ”€ HDFS:/data/output/spark/
   â””â”€ MongoDB:bigdata.results
                         â”‚
                         â–¼
4. VISUALIZATION
   â”Œâ”€ REST APIs â”€â”€â–¶ JSON Data
   â”œâ”€ Web Dashboard â”€â”€â–¶ Interactive Charts  
   â””â”€ Export Functions â”€â”€â–¶ CSV/Excel

====================================================
                  COMMANDES D'EXÃ‰CUTION
====================================================

# 1. DÃ©marrage complet
docker-compose up --build -d
./scripts/run_all.sh

# 2. Initialisation Ã©tape par Ã©tape
./scripts/setup/init-hadoop.sh
./scripts/setup/init-spark.sh  
./scripts/setup/load-data.sh

# 3. Lancement des analyses
# Pig Analysis
docker exec hadoop-master pig scripts/pig/data_analysis.pig
docker exec hadoop-master pig scripts/pig/sales_analysis.pig

# Spark Analysis  
docker exec spark-master spark-submit scripts/spark/data_processing.py
docker exec spark-master spark-submit scripts/spark/mongodb_reader.py

# 4. AccÃ¨s aux interfaces
# Application Web: http://localhost:5000
# Hadoop NameNode: http://localhost:9870
# Spark Master: http://localhost:8080
# MongoDB Express: http://localhost:8090

====================================================
                MONITORING & LOGGING
====================================================

ğŸ“Š MÃ©triques surveillÃ©es:
â”œâ”€ HDFS: Utilisation stockage, santÃ© DataNodes
â”œâ”€ Spark: Applications actives, utilisation ressources  
â”œâ”€ MongoDB: Connexions, performances requÃªtes
â””â”€ Flask: Temps rÃ©ponse APIs, erreurs HTTP

ğŸ“ Logs disponibles:
â”œâ”€ docker logs [container_name]
â”œâ”€ HDFS: /opt/hadoop/logs/
â”œâ”€ Spark: /opt/bitnami/spark/logs/
â””â”€ Application: ./logs/run_all.log

ğŸ”„ SantÃ© systÃ¨me:
â””â”€ API /api/system/status (temps rÃ©el)

====================================================
Ce workflow assure un pipeline complet de traitement
Big Data avec redondance, scalabilitÃ© et monitoring.
====================================================