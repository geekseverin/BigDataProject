#!/bin/bash
# =====================================================
# SCRIPT D'INITIALISATION APACHE SPARK
# =====================================================
# Configure et teste le cluster Spark
# Utilisation: ./init-spark.sh

set -e  # ArrÃªter en cas d'erreur

echo "======================================================"
echo "âš¡ INITIALISATION DU CLUSTER SPARK"
echo "======================================================"

# Variables de configuration
SPARK_MASTER="spark-master"
SPARK_MASTER_URL="spark://spark-master:7077"
TIMEOUT=120

# Fonction d'attente de service
wait_for_service() {
    local host=$1
    local port=$2
    local service_name=$3
    local timeout=$4
    
    echo "â³ Attente du service $service_name sur $host:$port..."
    
    local count=0
    while ! nc -z $host $port 2>/dev/null; do
        if [ $count -ge $timeout ]; then
            echo "âŒ Timeout: $service_name non disponible aprÃ¨s ${timeout}s"
            return 1
        fi
        sleep 1
        ((count++))
    done
    
    echo "âœ… Service $service_name disponible"
}

# VÃ©rifier le statut du cluster Spark
check_spark_cluster() {
    echo "ðŸ” VÃ©rification du cluster Spark..."
    
    # VÃ©rifier que tous les conteneurs Spark sont actifs
    local spark_containers=("spark-master" "spark-worker-1" "spark-worker-2")
    
    for container in "${spark_containers[@]}"; do
        if ! docker ps --format "table {{.Names}}" | grep -q "^$container$"; then
            echo "âŒ Conteneur Spark $container non trouvÃ©"
            return 1
        fi
        echo "âœ… Conteneur $container actif"
    done
    
    # Attendre les services
    wait_for_service $SPARK_MASTER 8080 "Spark Master UI" 60
    wait_for_service $SPARK_MASTER 7077 "Spark Master" 60
    
    # VÃ©rifier les workers
    echo "ðŸ”„ VÃ©rification des workers Spark..."
    sleep 10
    
    local worker_count=$(docker exec spark-master curl -s http://localhost:8080 | grep -c "worker-" || echo "0")
    if [ "$worker_count" -lt 2 ]; then
        echo "âš ï¸  Seulement $worker_count workers dÃ©tectÃ©s, attendu 2"
        sleep 15  # Attendre un peu plus
        worker_count=$(docker exec spark-master curl -s http://localhost:8080 | grep -c "worker-" || echo "0")
    fi
    
    echo "âœ… $worker_count workers Spark connectÃ©s"
    return 0
}

# CrÃ©er les rÃ©pertoires Spark dans HDFS
setup_spark_hdfs_dirs() {
    echo "ðŸ“ CrÃ©ation des rÃ©pertoires Spark dans HDFS..."
    
    # VÃ©rifier que Hadoop est disponible
    if ! docker exec hadoop-master bash -c "su - hadoop -c '$HADOOP_HOME/bin/hdfs dfs -ls /'" > /dev/null 2>&1; then
        echo "âš ï¸  HDFS non disponible, initialisation requise"
        echo "ðŸ’¡ ExÃ©cutez d'abord: ./scripts/setup/init-hadoop.sh"
        return 1
    fi
    
    # CrÃ©er les rÃ©pertoires nÃ©cessaires
    local spark_dirs=(
        "/spark-logs"
        "/spark-checkpoints"
        "/spark-warehouse"
        "/user/spark"
    )
    
    for dir in "${spark_dirs[@]}"; do
        echo "   CrÃ©ation: $dir"
        docker exec hadoop-master bash -c "su - hadoop -c '$HADOOP_HOME/bin/hdfs dfs -mkdir -p $dir'" 2>/dev/null || true
        docker exec hadoop-master bash -c "su - hadoop -c '$HADOOP_HOME/bin/hdfs dfs -chmod 777 $dir'" 2>/dev/null || true
    done
    
    echo "âœ… RÃ©pertoires Spark crÃ©Ã©s dans HDFS"
}

# Tester les applications Spark
test_spark_applications() {
    echo "ðŸ§ª Test des applications Spark..."
    
    # Test 1: Application Pi simple
    echo "ðŸ“ Test calcul Pi..."
    
    local pi_result
    pi_result=$(docker exec spark-master bash -c "
        /opt/bitnami/spark/bin/spark-submit \
        --master $SPARK_MASTER_URL \
        --class org.apache.spark.examples.SparkPi \
        /opt/bitnami/spark/examples/jars/spark-examples*.jar \
        10 2>/dev/null | grep 'Pi is roughly' | tail -1
    " || echo "Erreur")
    
    if [[ "$pi_result" == *"Pi is roughly"* ]]; then
        echo "âœ… Test Pi rÃ©ussi: $pi_result"
    else
        echo "âŒ Test Pi Ã©chouÃ©"
        return 1
    fi
    
    # Test 2: Application PySpark simple
    echo "ðŸ Test PySpark..."
    
    # CrÃ©er un script Python de test
    docker exec spark-master bash -c "cat > /tmp/test_pyspark.py << 'EOF'
from pyspark.sql import SparkSession
import sys

spark = SparkSession.builder.appName('TestPySpark').getOrCreate()
data = [1, 2, 3, 4, 5]
rdd = spark.sparkContext.parallelize(data)
result = rdd.map(lambda x: x * 2).collect()
print(f'Test PySpark rÃ©ussi: {result}')
spark.stop()
EOF"
    
    local pyspark_result
    pyspark_result=$(docker exec spark-master bash -c "
        /opt/bitnami/spark/bin/spark-submit \
        --master $SPARK_MASTER_URL \
        /tmp/test_pyspark.py 2>/dev/null | grep 'Test PySpark rÃ©ussi' | tail -1
    " || echo "Erreur")
    
    if [[ "$pyspark_result" == *"Test PySpark rÃ©ussi"* ]]; then
        echo "âœ… Test PySpark rÃ©ussi: $pyspark_result"
    else
        echo "âŒ Test PySpark Ã©chouÃ©"
        return 1
    fi
    
    echo "âœ… Tous les tests Spark rÃ©ussis"
}

# Tester la connectivitÃ© HDFS depuis Spark
test_spark_hdfs_integration() {
    echo "ðŸ”— Test intÃ©gration Spark-HDFS..."
    
    # CrÃ©er un script de test d'intÃ©gration
    docker exec spark-master bash -c "cat > /tmp/test_hdfs_integration.py << 'EOF'
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName('TestSparkHDFS') \
    .config('spark.sql.adaptive.enabled', 'true') \
    .getOrCreate()

try:
    # Test lecture HDFS
    df = spark.read.option('header', 'true').csv('hdfs://hadoop-master:9000/data/input/sample_data.csv')
    count = df.count()
    print(f'Lecture HDFS rÃ©ussie: {count} lignes')
    
    # Test Ã©criture HDFS
    df.limit(5).write.mode('overwrite').option('header', 'true').csv('hdfs://hadoop-master:9000/tmp/spark_test')
    print('Ã‰criture HDFS rÃ©ussie')
    
    # Test analyse simple
    dept_count = df.groupBy('department').count().collect()
    print(f'Analyse dÃ©partements: {len(dept_count)} dÃ©partements')
    
    print('Test intÃ©gration Spark-HDFS: SUCCÃˆS')
    
except Exception as e:
    print(f'Erreur intÃ©gration Spark-HDFS: {str(e)}')
    
finally:
    spark.stop()
EOF"
    
    local integration_result
    integration_result=$(docker exec spark-master bash -c "
        /opt/bitnami/spark/bin/spark-submit \
        --master $SPARK_MASTER_URL \
        --conf spark.hadoop.fs.defaultFS=hdfs://hadoop-master:9000 \
        /tmp/test_hdfs_integration.py 2>/dev/null | grep -E '(Lecture HDFS|Ã‰criture HDFS|Test intÃ©gration)' | tail -3
    ")
    
    if [[ "$integration_result" == *"SUCCÃˆS"* ]]; then
        echo "âœ… IntÃ©gration Spark-HDFS fonctionnelle"
    else
        echo "âŒ ProblÃ¨me d'intÃ©gration Spark-HDFS"
        echo "RÃ©sultat: $integration_result"
        return 1
    fi
}

# Installer les dÃ©pendances Python supplÃ©mentaires
install_python_dependencies() {
    echo "ðŸ“¦ Installation des dÃ©pendances Python..."
    
    local dependencies=(
        "pymongo==4.5.0"
        "pandas==2.0.3"
        "numpy==1.24.3"
    )
    
    for dep in "${dependencies[@]}"; do
        echo "   Installation: $dep"
        docker exec spark-master bash -c "pip install $dep" > /dev/null 2>&1 || {
            echo "âš ï¸  Erreur installation $dep"
        }
    done
    
    echo "âœ… DÃ©pendances Python installÃ©es"
}

# Afficher les informations du cluster Spark
display_spark_info() {
    echo "======================================================"
    echo "ðŸ“Š INFORMATIONS DU CLUSTER SPARK"
    echo "======================================================"
    
    # Informations de base
    echo "ðŸ”— URLs d'accÃ¨s:"
    echo "   Spark Master UI:     http://localhost:8080"
    echo "   Spark Worker 1:      http://localhost:8081"
    echo "   Spark Worker 2:      http://localhost:8082"
    echo "   Spark History:       http://localhost:18080 (si activÃ©)"
    
    echo ""
    echo "âš™ï¸  Configuration Spark Master:"
    docker exec spark-master bash -c "
        echo 'Spark Version: '\$(/opt/bitnami/spark/bin/spark-submit --version 2>&1 | grep version | head -1)
        echo 'Master URL: $SPARK_MASTER_URL'
        echo 'UI Port: 8080'
    "
    
    echo ""
    echo "ðŸ‘¥ Workers connectÃ©s:"
    local worker_info
    worker_info=$(docker exec spark-master bash -c "curl -s http://localhost:8080/json/" | grep -o '"workers":\[.*\]' || echo "Impossible de rÃ©cupÃ©rer les informations")
    
    if [[ "$worker_info" != "Impossible"* ]]; then
        echo "   âœ… Workers dÃ©tectÃ©s dans l'interface"
    else
        echo "   âš ï¸  Information workers non disponible"
    fi
    
    echo ""
    echo "ðŸ—„ï¸  RÃ©pertoires HDFS Spark:"
    docker exec hadoop-master bash -c "su - hadoop -c '$HADOOP_HOME/bin/hdfs dfs -ls /spark* /user/spark'" 2>/dev/null || echo "   RÃ©pertoires en cours de crÃ©ation..."
    
    echo ""
    echo "ðŸ Python et dÃ©pendances:"
    docker exec spark-master bash -c "python3 --version && pip list | grep -E '(pyspark|pymongo|pandas|numpy)'" 2>/dev/null || echo "   Informations Python non disponibles"
}

# CrÃ©er des scripts d'exemple pour les tests
create_example_scripts() {
    echo "ðŸ“ CrÃ©ation de scripts d'exemple..."
    
    # Script d'exemple 1: Analyse simple
    docker exec spark-master bash -c "mkdir -p /opt/examples && cat > /opt/examples/simple_analysis.py << 'EOF'
#!/usr/bin/env python3
\"\"\"Exemple d'analyse simple avec Spark\"\"\"
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName('SimpleAnalysis').getOrCreate()

# Charger les donnÃ©es
df = spark.read.option('header', 'true').csv('hdfs://hadoop-master:9000/data/input/sample_data.csv')

# Analyses basiques
print('=== ANALYSE SIMPLE SPARK ===')
print(f'Total employÃ©s: {df.count()}')

# Par dÃ©partement
dept_stats = df.groupBy('department').agg(
    count('*').alias('count'),
    avg('salary').alias('avg_salary')
).orderBy(desc('count'))

print('EmployÃ©s par dÃ©partement:')
dept_stats.show()

spark.stop()
EOF"
    
    # Script d'exemple 2: Test MongoDB
    docker exec spark-master bash -c "cat > /opt/examples/mongodb_test.py << 'EOF'
#!/usr/bin/env python3
\"\"\"Test de connexion MongoDB\"\"\"
try:
    from pymongo import MongoClient
    client = MongoClient('mongodb://admin:bigdata2025@mongodb:27017/')
    db = client['bigdata']
    collections = db.list_collection_names()
    print(f'MongoDB connectÃ©. Collections: {collections}')
    client.close()
except Exception as e:
    print(f'Erreur MongoDB: {e}')
EOF"
    
    echo "âœ… Scripts d'exemple crÃ©Ã©s dans /opt/examples/"
}

# Fonction principale
main() {
    echo "âš¡ DÃ©marrage de l'initialisation Spark..."
    
    # VÃ©rifier les prÃ©requis
    if ! docker ps | grep -q spark-master; then
        echo "âŒ Conteneur Spark Master non dÃ©marrÃ©"
        echo "ðŸ’¡ ExÃ©cutez: docker-compose up -d spark-master"
        exit 1
    fi
    
    # SÃ©quence d'initialisation
    check_spark_cluster
    setup_spark_hdfs_dirs
    install_python_dependencies
    test_spark_applications
    test_spark_hdfs_integration
    create_example_scripts
    display_spark_info
    
    echo ""
    echo "======================================================"
    echo "ðŸŽ‰ CLUSTER SPARK INITIALISÃ‰ AVEC SUCCÃˆS!"
    echo "======================================================"
    echo "âœ… Spark Master opÃ©rationnel"
    echo "âœ… Workers connectÃ©s"
    echo "âœ… IntÃ©gration HDFS fonctionnelle"
    echo "âœ… PySpark configurÃ©"
    echo "âœ… DÃ©pendances installÃ©es"
    echo ""
    echo "ðŸš€ PrÃªt pour l'analyse Spark!"
    echo "ðŸ’¡ Prochaine Ã©tape: ./scripts/setup/load-data.sh"
    echo ""
    echo "ðŸ§ª Tests disponibles:"
    echo "   docker exec spark-master python3 /opt/examples/simple_analysis.py"
    echo "   docker exec spark-master python3 /opt/examples/mongodb_test.py"
}

# Point d'entrÃ©e
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi