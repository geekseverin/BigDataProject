# Configuration Spark par défaut
# Fichier de configuration principal pour Apache Spark 3.4+

# =====================================================
# CONFIGURATION CLUSTER SPARK
# =====================================================

# Master Spark - URL de connexion au cluster
spark.master                     spark://spark-master:7077

# Nom de l'application par défaut
spark.app.name                   BigDataProject

# =====================================================
# CONFIGURATION DES RESSOURCES
# =====================================================

# Configuration Driver (client qui soumet l'application)
spark.driver.memory              1g
spark.driver.cores               1
spark.driver.maxResultSize       512m

# Configuration Executor (processus qui exécutent les tâches)
spark.executor.memory            2g
spark.executor.cores             2
spark.executor.instances         2

# Configuration de la mémoire
spark.sql.adaptive.enabled       true
spark.sql.adaptive.coalescePartitions.enabled  true

# =====================================================
# CONFIGURATION HDFS ET STOCKAGE
# =====================================================

# Configuration pour accès HDFS
spark.hadoop.fs.defaultFS        hdfs://hadoop-master:9000
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://hadoop-master:9000/spark-logs

# Configuration des checkpoints
spark.sql.streaming.checkpointLocation  hdfs://hadoop-master:9000/spark-checkpoints

# =====================================================
# CONFIGURATION MONGODB
# =====================================================

# Connecteur MongoDB pour Spark
spark.mongodb.input.uri          mongodb://admin:bigdata2025@mongodb:27017/bigdata.sales
spark.mongodb.output.uri         mongodb://admin:bigdata2025@mongodb:27017/bigdata.results

# Driver MongoDB
spark.jars.packages              org.mongodb.spark:mongo-spark-connector_2.12:3.0.1

# =====================================================
# CONFIGURATION SÉRIALISATION
# =====================================================

# Sérialiseur Kryo pour de meilleures performances
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe                true

# =====================================================
# CONFIGURATION RÉSEAU ET SÉCURITÉ
# =====================================================

# Configuration réseau
spark.driver.host               0.0.0.0
spark.driver.bindAddress        0.0.0.0

# Ports réseau
spark.driver.port               7001
spark.blockManager.port         7002

# Configuration UI
spark.ui.enabled                true
spark.ui.port                   4040

# =====================================================
# CONFIGURATION SPARK SQL
# =====================================================

# Optimisations SQL
spark.sql.adaptive.enabled                    true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled          true

# Configuration des partitions
spark.sql.shuffle.partitions     200

# Support des formats de fichiers
spark.sql.parquet.compression.codec  snappy
spark.sql.parquet.enableVectorizedReader  true

# =====================================================
# CONFIGURATION STREAMING
# =====================================================

# Configuration Spark Streaming
spark.streaming.receiver.maxRate           10000
spark.streaming.kafka.maxRatePerPartition  1000

# =====================================================
# CONFIGURATION HISTORIQUE ET LOGS
# =====================================================

# Configuration des logs
spark.history.provider           org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory    hdfs://hadoop-master:9000/spark-logs
spark.history.fs.update.interval 10s
spark.history.ui.port            18080

# Nettoyage automatique des logs
spark.history.fs.cleaner.enabled true
spark.history.fs.cleaner.interval 1d
spark.history.fs.cleaner.maxAge   7d

# =====================================================
# CONFIGURATION PERFORMANCE
# =====================================================

# Configuration de la mémoire JVM
spark.driver.extraJavaOptions   -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1PrintGCDetails -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseCompressedOops
spark.executor.extraJavaOptions -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1PrintGCDetails -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseCompressedOops

# Configuration du cache
spark.storage.level              MEMORY_AND_DISK_SER
spark.rdd.compress               true

# Configuration broadcast
spark.sql.broadcastTimeout      300

# =====================================================
# CONFIGURATION DÉVELOPPEMENT
# =====================================================

# Mode développement
spark.sql.execution.arrow.pyspark.enabled  true
spark.sql.repl.eagerEval.enabled           true
spark.sql.repl.eagerEval.maxNumRows        20

# Configuration Python
spark.pyspark.driver.python     python3
spark.pyspark.python            python3